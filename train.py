"""
Based on code by xternalz: https://github.com/xternalz/WideResNet-pytorch
Wide ResNet by Sergey Zagoruyko and Nikos Komodakis
"""

import argparse
import os
import shutil
import time

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim
import torch.utils.data

import torchvision

import torchvision.transforms as transforms
import torchvision.datasets as datasets

from torch.autograd import Variable

from model import WideResNet
##from model import ResNet5
##from remodel import ConvNet as WideResNet
##from remodel import NarrowNet as WideResNet

from utils.cutout import Cutout
from utils.radam import RAdam, AdamW
from utils.imgnet import IMGNET


from tensorboard_logger import configure, log_value

from torch.utils.tensorboard import SummaryWriter
import numpy as np
from torchviz import make_dot

import prunhild


parser = argparse.ArgumentParser(description="PyTorch WideResNet Training")
parser.add_argument("--print-freq", "-p", default=10, type=int, help="default: 10")
parser.add_argument("--layers", default=28, type=int, help="default: 28")
parser.add_argument("--widen-factor", default=10, type=int, help="default: 10")
parser.add_argument("--batchnorm", default=False, help="apply BatchNorm",  action='store_true')
parser.add_argument("--fixup", default=False, help="apply Fixup", action='store_true'  )
parser.add_argument("--droprate", default=0, type=float, help="default: 0.0")
parser.add_argument("--cutout", default=False, type=bool, help="apply cutout")
parser.add_argument("--length", default=16, type=int, help="length of the holes")
parser.add_argument("--n_holes", default=1, type=int, help="number of holes to cut out")
parser.add_argument(
    "--dataset", default="cifar10", type=str, help="cifar10 [default], cifar100, cinic10, imgnet10 or imgnet100"
)




parser.add_argument("--epochs", default=200, type=int, help="default: 200")
parser.add_argument("--start-epoch", default=0, type=int, help="epoch for restart")
parser.add_argument("-b", "--batch-size", default=128, type=int, help="default: 128")

parser.add_argument('--optimizer', default='sgd', type=str, choices=['radam', 'sgd'])

parser.add_argument('--beta1', default=0.9, type=float,
                    help='beta1 for adam')
parser.add_argument('--beta2', default=0.999, type=float,
                    help='beta2 for adam')

parser.add_argument(
    "--lr", "--learning-rate", default=0.1, type=float, help="default: 0.1"
)
parser.add_argument("--momentum", default=0.9, type=float, help="momentum")
parser.add_argument("--nonesterov", action = "store_true" , help="nesterov momentum")
parser.add_argument(
    "--weight-decay", "--wd", default=5e-4, type=float, help="default: 5e-4"
)
parser.add_argument('--alpha', default=0.0, type=float,
                    help='mixup interpolation coefficient (default: 0.0)')


parser.add_argument('--forceW', default=0.0, type=float,
                    help='mixup interpolation coefficient (default: 0.0)')



parser.add_argument(
    "--resume", default="", type=str, help="path to latest checkpoint (default: '')"
)
parser.add_argument(
    "-d","--device", default="0", type=str, help="GPU Device"
)
parser.add_argument(
    "--name", default="WideResNet-28-10", type=str, help="name of experiment"
)

parser.add_argument(
    "--dir", default="/home/ehoffer/Datasets", type=str, help="dataset directory"
)

parser.add_argument(
    "--tensorboard", help="Log progress to TensorBoard", action="store_true"
)
parser.add_argument(
    "--no-augment",
    dest="augment",
    action="store_false",
    help="whether to use standard augmentation (default: True)",
)

parser.add_argument(
    "--no-saves",
    dest="savenet",
    action="store_false",
    help="whether to save networks weights (default: True)",
)

parser.add_argument(
    "--prune", default="", type=str, help="path to checkpoint to prone"
)

parser.add_argument(
    "--prune_epoch", default=0, type=int, help="base epoch to use weights from"
)

parser.add_argument(
    "--prune_classes", default="0", type=str, help="Number of classes in the source pruned network (0- same as current dataset)"
)

parser.add_argument("--randomize_mask", default=False, action='store_true' , help="Use random mask for pruning")


parser.add_argument(
    "--cutoff", default=0.15, type=float, help="ratio of weights to keep"
)

parser.add_argument("--eval", default=False, action='store_true' , help="Evaluation only")

parser.add_argument("--varnet", default=False, action='store_true' , help="Use diversed initialization")

parser.add_argument("--symmetry_break", default=False, action='store_true' , help="Quit if the accuracy is over 50% for some time")

parser.add_argument(
    "--noise", default=0.0, type=float, help="noise. 0.0: constnet, 1.0: varnet"
)

parser.add_argument(
    "--lrelu", default=0.0, type=float, help="leaky relu variable"
)


parser.add_argument(
    "-W", "--sigmaW", default=-1.0, type=float, help="Varnet parameter for initializing weights"
)

parser.add_argument(
    "--freeze", default=0, type=int, help="Number of top layers to freeze"
)

parser.add_argument(
    "--freeze_start", default=0, type=int, help="Number of top layers not to freeze"
)

parser.add_argument(
    "--res_freeze", default=0, type=int, help="Number of conv layers to freeze"
)

parser.add_argument(
    "--res_freeze_start", default=0, type=int, help="Number of conv layers not to freeze"
)

parser.add_argument(
    "--cudaNoise", default=True, action='store_false' , help="Turn cudann to deterministic"
)

parser.set_defaults(augment=True)

class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def mixup_data(x, y, alpha=1.0, use_cuda=True):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)




def draw(args,model):
    misc = torch.zeros([args.batch_size,3,32,32])
    dot = make_dot(model(misc), params= dict(model.named_parameters()))
    dot.format = 'pdf'
    if args.batchnorm:
        dot.render("batchnorm-graph") 
    elif args.fixup:
        dot.render("fixup-graph")
    else:
        dot.render("no-fixup-no-bn-graph")
    
def justParse(txt=None):
    if not txt:
        args = parser.parse_args()
    else:
        args = parser.parse_args(txt.split())

    if args.sigmaW == -1.0:
        if args.varnet:
            args.sigmaW = 1.0
        else:
            args.sigmaW = 0.0

    return args

def intersection(lst1, lst2): 
    lst3 = [value for value in lst1 if value in lst2] 
    return lst3 

def prune_on(args,str):
    if intersection(['bn','scale','conv_res','fc'],str.split('.')):
        return False
    return True

def get_params_for_pruning(args,model):
    return [param for param,name in zip(list(model.parameters()),list(model.named_parameters())) if prune_on(args,name[0])]

def count_pruned_weights(parameters, cutoff = 1.0):
    return int(np.sum([param.nelement() for  param in parameters]) * (cutoff))

def randomize_mask(mask,cutoff):
    st = mask['state']
    for k in st.keys():
        m = st[k]['prune_mask']
        mshape = (m.shape)
        m.data = torch.bernoulli(cutoff * torch.ones(mshape))
    return mask

def getPruneMask(args):
    baseTar =  "runs/%s-net/checkpoint.pth.tar" % args.prune
    if os.path.isfile(baseTar):
        

        classes = onlydigits(args.prune_classes)
        if classes == 0:
            classes = args.classes

        fullModel = WideResNet(
            args.layers,
            classes,
            args.widen_factor,
            droprate=args.droprate,
            use_bn=args.batchnorm,
            use_fixup=args.fixup,
            varnet = args.varnet,
            noise = args.noise,
            lrelu = args.lrelu,
            sigmaW = args.sigmaW,
        )


        if torch.cuda.device_count() > 1:
            
            start = int(args.device[0])
            end  = int(args.device[2])+1
            torch.cuda.set_device(start)
            dev_list=[]
            for i in range(start,end):
                dev_list.append("cuda:%d" % i)
            fullModel = torch.nn.DataParallel(fullModel, device_ids=dev_list)

        fullModel = fullModel.cuda()


        print(f"=> loading checkpoint {baseTar}")

        checkpoint = torch.load(baseTar)
        fullModel.load_state_dict(checkpoint["state_dict"])


        # --------------------------- #
        # --- Pruning Setup Start --- #

        cutoff = prunhild.cutoff.LocalRatioCutoff(args.cutoff)
        # don't prune the final bias weights
        params = get_params_for_pruning(args,fullModel)

        print(params)

        pruner = prunhild.pruner.CutoffPruner(params, cutoff, prune_online=True)
        pruner.prune()
       
        print(f"=> loaded checkpoint '{baseTar}' (epoch {checkpoint['epoch']})")

        if torch.cuda.device_count() > 1:
            start = int(args.device[0])
            end  = int(args.device[2])+1
            for i in range(start,end):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()

        mask = pruner.state_dict()
        if args.randomize_mask:
            mask = randomize_mask(mask,args.cutoff)

        return mask
    else:
        print(f"=> no checkpoint found at {baseTar}")
        return None
        
def main(txt=None):
    args = justParse(txt)

    return main2(args)

def nondigits(txt):
    return ''.join([i for i in txt if not i.isdigit()])

def onlydigits(txt):
    return int(''.join([i for i in txt if i.isdigit()]))





def main2(args):
    best_prec1 = 0.0

    torch.backends.cudnn.deterministic = not args.cudaNoise

    if args.tensorboard:
        configure(f"runs/{args.name}")

    dstype = nondigits(args.dataset)
    if dstype ==  "cifar":
        means = [125.3, 123.0, 113.9]
        stds =  [63.0, 62.1, 66.7]
    elif dstype ==  "imgnet":
        means = [123.3, 118.1, 108.0]
        stds =  [54.1, 52.6, 53.2]

    normalize = transforms.Normalize(
        mean=[x / 255.0 for x in means],
        std=[x / 255.0 for x in stds],
    )


    writer = SummaryWriter(log_dir="runs/%s" % args.name  ,comment=str(args))
    args.classes =  onlydigits(args.dataset)


    if args.augment:
        transform_train = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Lambda(
                    lambda x: F.pad(
                        x.unsqueeze(0), (4, 4, 4, 4), mode="reflect"
                    ).squeeze()
                ),
                transforms.ToPILImage(),
                transforms.RandomCrop(32),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                normalize,
            ]
        )
    else:
        transform_train = transforms.Compose([transforms.ToTensor(), normalize])

    if args.cutout:
        transform_train.transforms.append(
            Cutout(n_holes=args.n_holes, length=args.length)
        )

    transform_test = transforms.Compose([transforms.ToTensor(), normalize])

    kwargs = {"num_workers": 1, "pin_memory": True}


    assert dstype in ["cifar","cinic","imgnet"]

    if dstype == "cifar": 
        train_loader = torch.utils.data.DataLoader(
            datasets.__dict__[args.dataset.upper()](
                "../data", train=True, download=True, transform=transform_train
            ),
            batch_size=args.batch_size,
            shuffle=True,
            **kwargs,
        )
        val_loader = torch.utils.data.DataLoader(
            datasets.__dict__[args.dataset.upper()](
                "../data", train=False, transform=transform_test
            ),
            batch_size=args.batch_size,
            shuffle=True,
            **kwargs,
        )
    elif  dstype == "cinic":
        cinic_directory = "%s/cinic10" % args.dir
        cinic_mean = [0.47889522, 0.47227842, 0.43047404]
        cinic_std = [0.24205776, 0.23828046, 0.25874835]
        train_loader = torch.utils.data.DataLoader(
            torchvision.datasets.ImageFolder(cinic_directory + '/train',
                transform=transforms.Compose([transforms.ToTensor(),
                transforms.Normalize(mean=cinic_mean,std=cinic_std)])),
            batch_size=args.batch_size, shuffle=True, **kwargs,)
        print("Using CINIC10 dataset")
        val_loader = torch.utils.data.DataLoader(
            torchvision.datasets.ImageFolder(cinic_directory + '/valid',
                transform=transforms.Compose([transforms.ToTensor(),
                transforms.Normalize(mean=cinic_mean,std=cinic_std)])),
            batch_size=args.batch_size, shuffle=True, **kwargs,)
    elif dstype == "imgnet":
        print("Using converted imagenet")
        train_loader = torch.utils.data.DataLoader(
            IMGNET("%s" % args.dir, train=True, transform=transform_train, target_transform=None, classes = args.classes),
            batch_size=args.batch_size,
            shuffle=True,
            **kwargs,
        )
        val_loader = torch.utils.data.DataLoader(
            IMGNET("%s" % args.dir, train=False, transform=transform_test, target_transform=None, classes = args.classes),
            batch_size=args.batch_size,
            shuffle=True,
            **kwargs,
        )
    else:
        print("Error matching dataset %s" % dstype)

    ##print("main bn:")
    ##print(args.batchnorm)
    ##print("main fixup:")
    ##print(args.fixup)

    if args.prune:
        pruner_state = getPruneMask(args)
        if pruner_state is None:
            print("Failed to prune network, aborting")
            return None


    model = WideResNet(
        args.layers,
        args.classes,
        args.widen_factor,
        droprate=args.droprate,
        use_bn=args.batchnorm,
        use_fixup=args.fixup,
        varnet = args.varnet,
        noise = args.noise,
        lrelu = args.lrelu,
        sigmaW = args.sigmaW,
    )
    
    draw(args,model)
    
    param_num = sum([p.data.nelement() for p in model.parameters()])
    
    print(f"Number of model parameters: {param_num}")

    if torch.cuda.device_count() > 1:
        
        start = int(args.device[0])
        end  = int(args.device[2])+1
        torch.cuda.set_device(start)
        dev_list=[]
        for i in range(start,end):
            dev_list.append("cuda:%d" % i)
        model = torch.nn.DataParallel(model, device_ids=dev_list)

    model = model.cuda()

    if args.freeze>0:
        cnt = 0
        for name,param in model.named_parameters():
            if intersection(['scale'],name.split('.')):
                cnt=cnt+1
                if cnt == args.freeze:
                    break

            if cnt >= args.freeze_start:
##                if intersection(['conv','conv1'],name.split('.')):
##                    print("Freezing Block: %s" % name.split('.')[1:3]  )
                if not intersection(['conv_res','fc'],name.split('.')):
                    param.requires_grad = False
                    print("Freezing Block: %s" % name)


    elif args.freeze < 0:
        cnt = 0
        for name,param in model.named_parameters():
            if intersection(['scale'],name.split('.')):
                cnt=cnt+1

            if cnt >  args.layers - 3 + args.freeze - 1:
##                if intersection(['conv','conv1'],name.split('.')):
##                    print("Freezing Block: %s" % name  )

                if not intersection(['conv_res','fc'],name.split('.')):
                    param.requires_grad = False
                    print("Freezing Block: %s" % name  )


    if args.res_freeze > 0:
        cnt = 0
        for name,param in model.named_parameters():
            if intersection(['conv_res'],name.split('.')):
                cnt=cnt+1
                if cnt > args.res_freeze_start:
                    param.requires_grad = False
                    print("Freezing Block: %s" % name)
                if cnt >= args.res_freeze:
                    break
    elif args.res_freeze < 0:
        cnt = 0
        for name,param in model.named_parameters():
            if intersection(['conv_res'],name.split('.')):
                cnt=cnt+1
                if cnt > 3 + args.res_freeze:
                    param.requires_grad = False
                    print("Freezing Block: %s" % name)


    if args.prune: 
        if  args.prune_epoch >= 100:
            weightsFile =  "runs/%s-net/checkpoint.pth.tar" % args.prune
        else:
            weightsFile =  "runs/%s-net/model_epoch_%d.pth.tar" % (args.prune, args.prune_epoch)

        if os.path.isfile(weightsFile):
            print(f"=> loading checkpoint {weightsFile}")
            checkpoint = torch.load(weightsFile)
            model.load_state_dict(checkpoint["state_dict"])
            print(f"=> loaded checkpoint '{weightsFile}' (epoch {checkpoint['epoch']})")
        else:
            if args.prune_epoch == 0:
                print(f"=> No source data, Restarting network from scratch")
            else:
                print(f"=> no checkpoint found at {weightsFile}, aborting...")
                return None


    else:
        if args.resume:
            tarfile = "runs/%s-net/checkpoint.pth.tar" % args.resume 
            if os.path.isfile(tarfile):
                print(f"=> loading checkpoint {args.resume}")
                checkpoint = torch.load(tarfile)
                args.start_epoch = checkpoint["epoch"]
                best_prec1 = checkpoint["best_prec1"]
                model.load_state_dict(checkpoint["state_dict"])
                print(f"=> loaded checkpoint '{tarfile}' (epoch {checkpoint['epoch']})")
            else:
                print(f"=> no checkpoint found at {tarfile}, aborting...")
                return None

    cudnn.benchmark = True
    criterion = nn.CrossEntropyLoss().cuda()

    if args.optimizer.lower() == 'sgd':
        optimizer = torch.optim.SGD(
            model.parameters(),
            args.lr,
            momentum=args.momentum,
            nesterov=(not args.nonesterov),
            weight_decay=args.weight_decay,
        )
    elif args.optimizer.lower() == 'radam':
        optimizer = RAdam(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2), weight_decay=args.weight_decay)

    if args.prune and pruner_state is not None:
        cutoff_retrain = prunhild.cutoff.LocalRatioCutoff(args.cutoff)
        params_retrain = get_params_for_pruning(args,model)
        pruner_retrain = prunhild.pruner.CutoffPruner(params_retrain, cutoff_retrain)
        pruner_retrain.load_state_dict(pruner_state) 
        pruner_retrain.prune(update_state=False)
        pruned_weights_count = count_pruned_weights(params_retrain, args.cutoff)
        params_left = param_num - pruned_weights_count
        print("Pruned %d weights, New model size:  %d/%d (%d%%)"  % (pruned_weights_count, params_left ,param_num, int(100*params_left/param_num) ))
        
    else:
        pruner_retrain = None

    if args.eval:
        best_prec1 = validate(args,val_loader, model, criterion, 0,None)
    else:

        if args.varnet:
            save_checkpoint(args,
                {
                    "epoch": 0,
                    "state_dict": model.state_dict(),
                    "best_prec1": 0.0,
                },
                True,
            )
            best_prec1 = 0.0


        turns_above_50=0

        for epoch in range(args.start_epoch, args.epochs):
            adjust_learning_rate(args,optimizer, epoch + 1)
            train(args,train_loader, model, criterion, optimizer, epoch, pruner_retrain, writer)

            prec1 = validate(args,val_loader, model, criterion, epoch,writer)



            is_best = prec1 > best_prec1
            best_prec1 = max(prec1, best_prec1)

            if args.savenet:
                save_checkpoint(args,
                    {
                        "epoch": epoch + 1,
                        "state_dict": model.state_dict(),
                        "best_prec1": best_prec1,
                    },
                    is_best,
                )

            if args.symmetry_break:
                if prec1 > 50.0:
                    turns_above_50+=1
                    if turns_above_50>3:
                        return epoch

    writer.close()

    print("Best accuracy: ", best_prec1)
    return best_prec1


def train(args,train_loader, model, criterion, optimizer, epoch, pruner, writer):
    """Train for one epoch on the training set"""
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    

    model.train()
    total =0 
    correct = 0
    reg_loss = 0.0
    train_loss = 0.0
    end = time.time()
    for i, (inputs, target) in enumerate(train_loader):

        target = target.cuda()
        inputs = inputs.cuda()
        
        inputs, targets_a, targets_b, lam = mixup_data(inputs, target, args.alpha, True)
        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))

        ##input_var = torch.autograd.Variable(input)
        ##target_var = torch.autograd.Variable(target)

        outputs = model(inputs)
        ##loss = criterion(output, target_var)
        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
##        print("loss:")
##        print(loss)
##        print(loss.item())
##        train_loss += loss.data[0]
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += target.size(0)
        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()
                    + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())

##        prec1 = accuracy(output.data, target, topk=(1,))[0]
##        losses.update(loss.data.item(), input.size(0))
##        top1.update(prec1.item(), input.size(0))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if pruner is not None:
            pruner.prune(update_state=False)

        batch_time.update(time.time() - end)
        end = time.time()

        if 0:
            if i % args.print_freq == 0:
                print(
                    f"Epoch: [{epoch}][{i}/{len(train_loader)}]\t"
                    f"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t"
                    f"Loss {losses.val:.4f} ({losses.avg:.4f})\t"
                    f"Prec@1 {top1.val:.3f} ({top1.avg:.3f})"
                )
            niter = epoch*len(train_loader)+i

    batch_idx = i
    if writer is not None:
        writer.add_scalar('Train/Loss', train_loss/batch_idx, epoch)
        writer.add_scalar('Train/Prec@1', 100.*correct/total, epoch)
##        writer.add_scalar('Train/RegLoss', reg_loss/batch_idx, niter)




def validate(args,val_loader, model, criterion, epoch, writer, quiet=False):
    """Perform validation on the validation set"""
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    model.eval()

    end = time.time()
    for i, (input, target) in enumerate(val_loader):
        target = target.cuda()
        input = input.cuda()
        input_var = Variable(input)
        target_var = Variable(target)

        with torch.no_grad():
            output = model(input_var)
        loss = criterion(output, target_var)

        prec1 = accuracy(output.data, target, topk=(1,))[0]
        losses.update(loss.data.item(), input.size(0))
        top1.update(prec1.item(), input.size(0))

        batch_time.update(time.time() - end)
        end = time.time()

        if i % args.print_freq == 0:
            if 0:
                print(
                    f"Test: [{i}/{len(val_loader)}]\t"
                    f"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t"
                    f"Loss {losses.val:.4f} ({losses.avg:.4f})\t"
                    f"Prec@1 {top1.val:.3f} ({top1.avg:.3f})"
                )
            niter = epoch*len(val_loader)+i
            ##writer.add_scalar('Test/Loss', losses.val, niter)
            ##writer.add_scalar('Test/Prec@1', top1.val, niter)

    if writer is not None:
        writer.add_scalar('AvgTest/Prec@1', top1.avg, epoch)
        writer.add_scalar('AvgTest/Loss', losses.avg, epoch)
        if args.tensorboard:
            log_value("val_loss", losses.avg, epoch)
            log_value("val_acc", top1.avg, epoch)


    if not quiet:
        print(f" * Prec@1 {top1.avg:.3f}")

    return top1.avg


def save_checkpoint(args,state, is_best, filename="checkpoint.pth.tar"):
    """Saves checkpoint to disk"""
    directory = "runs/%s-net/" % (args.name)

    if not os.path.exists(directory):
        os.makedirs(directory)


    epoch = state['epoch']

    filename = directory + filename
    torch.save(state, filename)

    if is_best:
        shutil.copyfile(filename, "runs/%s-net/" % (args.name) + "model_best.pth.tar")

    if epoch==0 or epoch==2:
        shutil.copyfile(filename, "runs/%s-net/" % (args.name) + "model_epoch_%d.pth.tar" % epoch )


def adjust_learning_rate(args,optimizer, epoch):
    """Sets the learning rate to the initial LR divided by 5 at 60th, 120th and 160th epochs"""
    lr = args.lr * (
        (0.2 ** int(epoch >= 60))
        * (0.2 ** int(epoch >= 120))
        * (0.2 ** int(epoch >= 160))
    )

    if args.tensorboard:
        log_value("learning_rate", lr, epoch)

    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def accuracy(output, target, topk=(1,)):
    """Computes the precision@k for the specified values of k"""
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))

    return res


if __name__ == "__main__":
    main()
